{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction into MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(based on https://towardsdatascience.com/a-beginners-introduction-into-mapreduce-2c912bb5e6ac)\n",
    "\n",
    "Many times, as Data Scientists, we have to deal with huge amount of data. In those cases, many approaches won’t work or won’t be feasible. A massive amount of data is good, it’s very good, and we want to utilize as much as possible.\n",
    "\n",
    "Here I want to introduce the MapReduce technique, which is a broad technique that is used to handle a huge amount of data. There are many implementations of MapReduce, including the famous Apache Hadoop. Here, I won’t talk about implementations. I’ll try to introduce the concept in the most intuitive way and present examples for both toy and real-life examples.\n",
    "\n",
    "Let’s start with some straightforward task. You’re given a list of strings, and you need to return the longest string. It’s pretty easy to do in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_longest_string(list_of_strings):\n",
    "    longest_string = None\n",
    "    longest_string_len = 0\n",
    "    for s in list_of_strings:\n",
    "        if len(s) > longest_string_len:\n",
    "            longest_string_len = len(s)\n",
    "            longest_string = s\n",
    "    return longest_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We go over the strings one by one, compute the length and keep the longest string until we finished.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even for lists with much more than 3 elements it works pretty well, here we try with 3000 elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n"
     ]
    }
   ],
   "source": [
    "list_of_strings = ['abc', 'python', 'dima']\n",
    "\n",
    "large_list_of_strings = list_of_strings*1000\n",
    "\n",
    "print(find_longest_string(large_list_of_strings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we try for 300 million elements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_list_of_strings = list_of_strings*100000000\n",
    "\n",
    "max_length = max(large_list_of_strings, key=len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our solution is not scalable. We can do “Horizontal Scaling”, we’ll design our code so it could run in parallel, and it will get much faster when we’ll add more processors and/or CPUs.\n",
    "\n",
    "To do that, we need to break our code into smaller components and see how we can execute computations in parallel. The intuition is as follows: 1) break our data into many chunks, 2) execute the `find_longest_string` function for every chunk in parallel and 3) find the longest string among the outputs of all chunks.\n",
    "\n",
    "Our code is very specific and it hard to break and modify, so instead of using the `find_longest_string` function, we’ll develop a more generic framework that will help us perform different computations in parallel on large data.\n",
    "\n",
    "The two main things we do in our code is computing the `len` of the string and comparing it to the longest string until now. We’ll break our code into two steps: 1) compute the `len` of all strings and 2) select the `max` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('python', 6)\n"
     ]
    }
   ],
   "source": [
    "# step 1:\n",
    "list_of_string_lens = [len(s) for s in large_list_of_strings]\n",
    "list_of_string_lens = zip(list_of_strings, list_of_string_lens)\n",
    "\n",
    "#step 2:\n",
    "max_len = max(list_of_string_lens, key=lambda t: t[1])\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(I’m calculating the length of the strings and then `zip` them together because this is much faster than doing it in one line and duplicating the list of strings)\n",
    "\n",
    "Now our “step 2” gets as input not the original list of strings, but some preprocessed data. This allows us to execute step two using the output of another “step two”s! We’ll understand that better in a bit, but first, let’s give those steps a name. We’ll call “step one” a “mapper” because it maps some value into some other value, and we’ll call “step two” a reducer because it gets a list of values and produces a single (in most cases) value. Here’re two helper functions for mapper and reducer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = len\n",
    "\n",
    "def reducer(p, c):\n",
    "    if p[1] > c[1]:\n",
    "        return p\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapper is just the `len` function. It gets a string and returns its length. The reducer gets two tuples as input and returns the one with the biggest length.\n",
    "\n",
    "Let’s rewrite our code using `map` and `reduce`, there are even built-in functions for this in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('python', 6)\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "#step 1\n",
    "mapped = map(mapper, large_list_of_strings)\n",
    "mapped = zip(large_list_of_strings, mapped)\n",
    "\n",
    "#step 2:\n",
    "reduced = reduce(reducer, mapped)\n",
    "print(reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code does exactly the same thing, it looks bit fancier, but also it is more generic and will help us parallelize it. Let’s look more closely at it:\n",
    "\n",
    "Step 1 maps our list of strings into a list of tuples using the mapper function (here I use the `zip` again to avoid duplicating the strings).\n",
    "\n",
    "Step 2 uses the reducer function, goes over the tuples from step one and applies it one by one. The result is a tuple with the maximum length.\n",
    "\n",
    "Now let's break our input into chunks and understand how it works before we do any parallelization (we’ll use the `chunkify` that breaks a large list into chunks of equal size):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('python', 6)\n"
     ]
    }
   ],
   "source": [
    "def chunkify(the_list, number_of_chunks):\n",
    "    chunk_size = len(the_list) // number_of_chunks\n",
    "   \n",
    "    chunk_list = [the_list[i:i + chunk_size] for i in range(0, len(the_list), chunk_size)] \n",
    "    return chunk_list\n",
    "\n",
    "data_chunks = chunkify(large_list_of_strings, number_of_chunks=4)\n",
    "\n",
    "#step 1:\n",
    "reduced_all = []\n",
    "for chunk in data_chunks:\n",
    "    mapped_chunk = map(mapper, chunk)\n",
    "    mapped_chunk = zip(chunk, mapped_chunk)\n",
    "    \n",
    "    reduced_chunk = reduce(reducer, mapped_chunk)\n",
    "    reduced_all.append(reduced_chunk)\n",
    "    \n",
    "#step 2:\n",
    "reduced = reduce(reducer, reduced_all)\n",
    "print(reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In step one, we go over our chunks and find the longest string in that chunk using a map and reduce. In step two, we take the output of step one, which is a list of reduced values, and perform a final reduce to get the longest string. We use `number_of_chunks=4` because this is the number of CPUs I have on my machine.\n",
    "\n",
    "We are almost ready to run our code in parallel. The only thing that we can do better is to add the first `reduce` step into a single mapper. We do that because we want to break our code into two simple steps and as the first `reduce` works on a single chunk and we want to parallelize it as well. This is how it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('python', 6)\n"
     ]
    }
   ],
   "source": [
    "def chunks_mapper(chunk):\n",
    "    mapped_chunk = map(mapper, chunk) \n",
    "    mapped_chunk = zip(chunk, mapped_chunk)\n",
    "    return reduce(reducer, mapped_chunk)\n",
    "\n",
    "data_chunks = chunkify(large_list_of_strings, number_of_chunks=4)\n",
    "\n",
    "#step 1:\n",
    "mapped = map(chunks_mapper, data_chunks)\n",
    "\n",
    "#step 2:\n",
    "reduced = reduce(reducer, mapped)\n",
    "\n",
    "print(reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a nice looking two steps code. Now we can parallelize step 1 using the `multiprocessing` module simply by using the `pool.map` function instead of the regular `map` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('python', 6)\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "pool = Pool(4)\n",
    "\n",
    "data_chunks = chunkify(large_list_of_strings, number_of_chunks=4)\n",
    "\n",
    "#step 1:\n",
    "mapped = pool.map(chunks_mapper, data_chunks)\n",
    "\n",
    "#step 2:\n",
    "reduced = reduce(reducer, mapped)\n",
    "print(reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our architecture is built using two functions: `map` and `reduce`. Each computation unit maps the input data and executes the initial reduce. Finally, some centralized unit executes the final reduce and returns the output. It looks like this:\n",
    "\n",
    "![Map-Reduce structure](mapreducestructure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This architecture has two important advantages:\n",
    "\n",
    "1. It is scalable: if we have more data, the only thing we need to do is to add more processing units. No code change needed!\n",
    "2. It is generic: this architecture supports a vast variety of tasks, we can replace our `map` and `reduce` function with almost anything and this way computer many different things in a scalable way.\n",
    "\n",
    "It is important to note that in most cases, our data will be very big and static. It means the breaking into chunks every time is inefficient and actually redundant. So in most applications in real life, we’ll store our data in chunks (or shards) from the very beginning. Then, we’ll be able to do different computations using the MapReduce technique.\n",
    "\n",
    "Now let's see a more interesting example: Word Count!\n",
    "\n",
    "Say we have a very big set of news articles and we want to find the top 10 used words not including stop words, how would we do that? First, let's get the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import os\n",
    "data_home = os.path.join(os.getcwd(), 'scikit_learn_data')\n",
    "news = fetch_20newsgroups(data_home=data_home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this post, I made the data 5 times larger so we could see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = news.data*5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each text in the dataset, we want to tokenize it, clean it, remove stop words and finally count the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/emilie/Documents/00\n",
      "[nltk_data]     0_academic/002_NUP/001_BigDataAnalytics/2021/000_pract\n",
      "[nltk_data]     ical/python/DataAnalyticsWithPython/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "nltk_data_folder = os.path.join(os.getcwd(), 'nltk_data')\n",
    "os.environ['NLTK_DATA'] = nltk_data_folder\n",
    "\n",
    "# importing stopwors from nltk library\n",
    "import nltk\n",
    "nltk.download('stopwords', download_dir=nltk_data_folder)\n",
    "from nltk.corpus import stopwords\n",
    "ENGLISH_STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def clean_word(word):\n",
    "    return re.sub(r'[^\\w\\s]','',word).lower()\n",
    "\n",
    "def word_not_in_stopwords(word):\n",
    "    return word not in ENGLISH_STOP_WORDS and word and word.isalpha()\n",
    "   \n",
    "    \n",
    "def find_top_words(data):\n",
    "    cnt = Counter()\n",
    "    for text in data:\n",
    "        tokens_in_text = text.split()\n",
    "        tokens_in_text = map(clean_word, tokens_in_text)\n",
    "        tokens_in_text = filter(word_not_in_stopwords, tokens_in_text)\n",
    "        cnt.update(tokens_in_text)\n",
    "        \n",
    "    return cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see how much time does it take without MapReduce:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20 s, sys: 3.24 ms, total: 20 s\n",
      "Wall time: 20 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('subject', 61260),\n",
       " ('lines', 59120),\n",
       " ('organization', 55925),\n",
       " ('would', 44345),\n",
       " ('one', 43235),\n",
       " ('writes', 39180),\n",
       " ('article', 33770),\n",
       " ('people', 29160),\n",
       " ('dont', 29065),\n",
       " ('like', 28785)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time find_top_words(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s write our `mapper`, `reducer` and `chunk_mapper`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(text):\n",
    "    tokens_in_text = text.split()\n",
    "    tokens_in_text = map(clean_word, tokens_in_text)\n",
    "    tokens_in_text = filter(word_not_in_stopwords, tokens_in_text)\n",
    "    return Counter(tokens_in_text)\n",
    "\n",
    "def reducer(cnt1, cnt2):\n",
    "    cnt1.update(cnt2)\n",
    "    return cnt1\n",
    "\n",
    "def chunk_mapper(chunk):\n",
    "    mapped = map(mapper, chunk)\n",
    "    reduced = reduce(reducer, mapped)\n",
    "    return reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mapper` gets a text, splits it into tokens, cleans them and filters stop words and non-words, finally, it counts the words within this single text document. The `reducer` function gets 2 counters and merges them. The `chunk_mapper` gets a chunk and does a MapReduce on it. Now let’s run using the framework we built it and see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('subject', 61260), ('lines', 59120), ('organization', 55925), ('would', 44345), ('one', 43235), ('writes', 39180), ('article', 33770), ('people', 29160), ('dont', 29065), ('like', 28785)]\n",
      "CPU times: user 495 ms, sys: 483 ms, total: 978 ms\n",
      "Wall time: 9.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pool = Pool(4)\n",
    "\n",
    "data_chunks = chunkify(data, number_of_chunks=4)\n",
    "\n",
    "#step 1:\n",
    "mapped = pool.map(chunk_mapper, data_chunks)\n",
    "\n",
    "#step 2:\n",
    "reduced = reduce(reducer, mapped)\n",
    "\n",
    "print(reduced.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much faster! Here, we were able to really utilize our computational power because the task is much more complex and requires more.\n",
    "\n",
    "To sum up, MapReduce is an exciting and essential technique for large data processing. It can handle a tremendous number of tasks including Counts, Search, Supervised and Unsupervised learning and more. Today there’s a lot of implementations and tools that can make our lives much more comfortable, but I think it is very important to understand the basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
