{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary and Multi-Class Classification\n",
    "\n",
    "In this tutorial we will explore how to perform binary and multi-class classification using Python, Pandas and Scikit-Learn on two sample datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Imports\n",
    "\n",
    "Import the Pandas library to load and manipulate tabular datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification\n",
    "Load the dataset from the locally available file `SAHeart.csv`, which contains a retrospective sample of males in a heart-disease high-risk region\n",
    "of the Western Cape, South Africa. A description of the data is available here: https://web.stanford.edu/~hastie/ElemStatLearn/datasets/SAheart.info.txt\n",
    "\n",
    "The result of loading the data using the Pandas `read_csv` function is a Pandas `DataFrame` named `heart`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart = pd.read_csv('SAheart.csv', sep=',', header=0)\n",
    "heart.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the columns of the `heart` Pandas `DataFrame` into:\n",
    "- `X`: the feature variables, columns 0 to 9 (`:9`) (sbp, tobacco, ldl, adiposity, famhist, typea, obesity, alcohol, age)\n",
    "- `y`: the response variable, column `9` (chd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = heart.iloc[:,:9]\n",
    "y = heart.iloc[:,9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the rows that we will use to *quickly* check the prediction of the classification methods (`460:`, i.e., rows from 460 to the end of the DataFrame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.iloc[460:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import `LogisticRegression` from the SciKit-Learn library to perform logistic regression.\n",
    "More details about this can be found online in the documentation for SciKit-Learn: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `LogisticRegression` object to configure how to perform the logistic regression.\n",
    "\n",
    "The `random_state` parameter make usre we get the same exact results every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr', max_iter=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the dataset in the `X` and `y` sub-dataframes to train and create the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quickly test the model predictions for the last two rows in the dataset (rows 460 and 461). It seems that the first prediction is incorrect while the second is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR.predict(X.iloc[460:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate the model performance by asking for the 0 to 1 `score` executed on the whole of the *training* dataset. \n",
    "Normally, two different sets of data are used for the purpose of training the model and the evaluation of the model, but here we keep it simple and use the same dataset for both training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(LR.score(X,y), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the general structure of the previous section, we now evaluate an SVM-based classifier.\n",
    "More details here: https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC\n",
    "\n",
    "We import the classifier class, create the SVM classifier `SVM` object to configure the SVM method (here with no parameters), fit the data to create the model, and show the quick prediction.\n",
    "\n",
    "*NOTE: Disregard the warning produced.*\n",
    "\n",
    "It seems that the first prediction is incorrect while the second is correct here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "SVM = svm.LinearSVC(random_state=0)\n",
    "SVM.fit(X, y)\n",
    "SVM.predict(X.iloc[460:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate the model performance by asking for the 0 to 1 `score` executed on the whole of the *training* dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(SVM.score(X,y), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forrest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the general structure of the previous section, we now evaluate a Random Forrest-based classifier.\n",
    "More details here: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "\n",
    "\n",
    "We import the classifier class, create the Random Forrest classifier `RF` object to configure the Random Forrest method, fit the data to create the model, and show the quick prediction.\n",
    "\n",
    "The prediction is correct for the first row but incorrect for the second here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
    "RF.fit(X, y)\n",
    "RF.predict(X.iloc[460:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate the model performance by asking for the 0 to 1 `score` executed on the whole of the *training* dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(RF.score(X,y), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What would happen if a larger `max_depth` value were used?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network / Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the general structure of the previous section, we now evaluate a Neural Network-based classifier, specifically a Multi-layer Perceptron.\n",
    "More details here: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "\n",
    "We import the classifier class, create the Neural Network classifier `NN` object to configure the MLP method, fit the data to create the model, and show the quick prediction.\n",
    "\n",
    "The prediction is correct for the first row but incorrect for the second here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "NN = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=0)\n",
    "NN.fit(X, y)\n",
    "NN.predict(X.iloc[460:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate the model performance by asking for the 0 to 1 `score` executed on the whole of the *training* dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(NN.score(X,y), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What would happen if a larger `hidden_layer_sizes` values were used?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "The methods evaluated here all have different performance and different advantages and disadvantages, but using Pandas and SciKit-Learn their use and evaluation is very similar and follows the same pattern of code:\n",
    "1. Import classifier\n",
    "2. Create a classifer object with optional configuration parameters\n",
    "3. Train the model on some data\n",
    "4. Evaluate the model on some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class Classification\n",
    "Load the training dataset from the locally available file `vowel.train.csv` and the evaluation (or *testing*) dataset from the `vowel.test.csv` file.\n",
    "These files contain data for speaker independent recognition of the eleven steady state vowels of British English.\n",
    "A description of the data is available here: https://web.stanford.edu/~hastie/ElemStatLearn/datasets/vowel.info.txt\n",
    "\n",
    "The result of loading the data using the Pandas `read_csv` function are two Pandas `DataFrame` objects:\n",
    "1. `vowel_train`: The DataFrame that contains the training subset of the data\n",
    "1. `vowel_test`: The DataFrame that contains the testing subset of the data\n",
    "\n",
    "As it can be seen, in the Multi-class classification example we will use separate datasets for training and evaluation/testing, as is normally done and in contrast to how it was done in the binary classification above.\n",
    "\n",
    "In this case, the data is already separated into two sub-sets but it would be possible to start with one dataset and split it into two subsets for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowel_train = pd.read_csv('vowel.train.csv', sep=',', header=0)\n",
    "vowel_test = pd.read_csv('vowel.test.csv', sep=',', header=0)\n",
    "\n",
    "vowel_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the columns of the training Pandas `DataFrame` into:\n",
    "- `X`: the feature variables, columns 1 to the last column (`1:`) (x.1, x.2, ..., x.10)\n",
    "- `y`: the response variable, column `0` (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = vowel_train.iloc[:,1:]\n",
    "y_tr = vowel_train.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the columns of the test Pandas `DataFrame` into:\n",
    "- `X`: the feature variables, columns 1 to the last column (`1:`) (x.1, x.2, ..., x.10)\n",
    "- `y`: the response variable, column `0` (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vowel_test.iloc[:,1:]\n",
    "y_test = vowel_test.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `LogisticRegression` from the SciKit-Learn library to perform logistic regression.\n",
    "\n",
    "Here the configuration and training of the model (the `fit` function call) are all done in one step.\n",
    "\n",
    "We create the Logistic Regression classifier object `LR` to configure the LR method, fit the **training** data to create the model, and predict the values of the **testing** dataset.\n",
    "\n",
    "The prediction is presentation is not as useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial', max_iter=300).fit(X_tr, y_tr)\n",
    "LR.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate the model performance by asking for the 0 to 1 `score` executed on the whole of the **testing** dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(LR.score(X_test,y_test), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the general structure of the previous section, we now evaluate an SVM-based classifier.\n",
    "More details here: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "The configuration and training of the model (the `fit` function call) are all done in one step.\n",
    "\n",
    "We create the SVM classifier object `SVM` to configure the SVM method and fit the **training** data to create the model.\n",
    "\n",
    "Since showing the predictions is not very useful, we only evaluate the model performance by asking for the 0 to 1 `score` executed on the whole of the **testing** dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = svm.SVC(decision_function_shape=\"ovo\", random_state=0).fit(X_tr, y_tr)\n",
    "round(SVM.score(X_test, y_test), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forrest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now evaluate a Random Forrest-based classifier.\n",
    "More details here: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "\n",
    "The configuration and training of the model (the `fit` function call) are all done in one step.\n",
    "\n",
    "We create the Random Forrest classifier `RF` object to configure the Random Forrest method and fit the **training** data to create the model.\n",
    "\n",
    "Since showing the predictions is not very useful, we only evaluate the model performance by asking for the 0 to 1 `score` executed on the whole of the **testing** dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(n_estimators=1000, max_depth=10, random_state=0).fit(X_tr, y_tr)\n",
    "round(RF.score(X_test, y_test), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network / Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now evaluate a Neural Network-based classifier, specifically a Multi-layer Perceptron.\n",
    "More details here: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "\n",
    "The configuration and training of the model (the `fit` function call) are all done in one step.\n",
    "\n",
    "We create the Neural Network classifier `NN` object to configure the MLP method and fit the **training** data to create the model.\n",
    "\n",
    "Since showing the predictions is not very useful, we only evaluate the model performance by asking for the 0 to 1 `score` executed on the whole of the **testing** dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(40, 10), random_state=0,max_iter=300).fit(X_tr, y_tr)\n",
    "round(NN.score(X_test, y_test), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Out of the methods evaluated here, the SVM-based classifier seems to perform best, but not by a large margin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
